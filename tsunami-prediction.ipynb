{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13229155,"sourceType":"datasetVersion","datasetId":8385511}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"ahmeduzaki/global-earthquake-tsunami-risk-assessment-dataset\")\n\nprint(\"Path to dataset files:\", path) #be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T21:51:28.402849Z","iopub.execute_input":"2025-10-15T21:51:28.403168Z","iopub.status.idle":"2025-10-15T21:51:28.990493Z","shell.execute_reply.started":"2025-10-15T21:51:28.403147Z","shell.execute_reply":"2025-10-15T21:51:28.989528Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/global-earthquake-tsunami-risk-assessment-dataset\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/global-earthquake-tsunami-risk-assessment-dataset/earthquake_data_tsunami.csv')\nprint(\"Complete\")\n\nX= df[['magnitude', 'cdi', 'mmi', 'sig', 'nst', 'dmin', 'gap', 'depth',\n                 'latitude', 'longitude', 'Year', 'Month']]\ny = 'tsunami'\n\ntrain_df, test_df = train_test_split(\n    df, test_size=0.25, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T21:51:28.992187Z","iopub.execute_input":"2025-10-15T21:51:28.992496Z","iopub.status.idle":"2025-10-15T21:51:29.008788Z","shell.execute_reply.started":"2025-10-15T21:51:28.992474Z","shell.execute_reply":"2025-10-15T21:51:29.007466Z"}},"outputs":[{"name":"stdout","text":"Complete\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"class CustomDatasetFromDataFrame(Dataset):\n    def __init__(self, dataframe, features_cols, target_col, mean=None, std=None):\n        # Change to float32 explicitly\n        self.features = torch.tensor(dataframe[features_cols].values, dtype=torch.float32)\n        self.targets = torch.tensor(dataframe[target_col].values, dtype=torch.long)\n        \n        if mean is not None and std is not None:\n            # Ensure mean and std are also float32\n            mean = torch.tensor(mean.values, dtype=torch.float32)\n            std = torch.tensor(std.values, dtype=torch.float32)\n            self.features = (self.features - mean) / (std + 1e-8)\n    \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.targets[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T21:51:29.009665Z","iopub.execute_input":"2025-10-15T21:51:29.010082Z","iopub.status.idle":"2025-10-15T21:51:29.018700Z","shell.execute_reply.started":"2025-10-15T21:51:29.010054Z","shell.execute_reply":"2025-10-15T21:51:29.017719Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\n\nfeatures_cols = ['magnitude', 'cdi', 'mmi', 'sig', 'nst', 'dmin', 'gap', 'depth',\n                 'latitude', 'longitude', 'Year', 'Month']\n\ntrain_mean = train_df[features_cols].mean()\ntrain_std = train_df[features_cols].std()\n\n\ntrain_std[train_std == 0] = 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T21:51:29.019874Z","iopub.execute_input":"2025-10-15T21:51:29.020312Z","iopub.status.idle":"2025-10-15T21:51:29.046549Z","shell.execute_reply.started":"2025-10-15T21:51:29.020279Z","shell.execute_reply":"2025-10-15T21:51:29.045336Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"features_cols = ['magnitude', 'cdi', 'mmi', 'sig', 'nst', 'dmin', 'gap', 'depth',\n                 'latitude', 'longitude', 'Year', 'Month']\ntarget_col = 'tsunami'\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target_col])\n\ntrain_mean = train_df[features_cols].mean()\ntrain_std = train_df[features_cols].std()\n\ntrain_dataset = CustomDatasetFromDataFrame(\n    train_df, \n    features_cols, \n    target_col, \n    mean=train_mean,\n    std=train_std\n)\ntest_dataset = CustomDatasetFromDataFrame(\n    test_df, \n    features_cols, \n    target_col, \n    mean=train_mean, \n    std=train_std\n)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T21:51:29.048694Z","iopub.execute_input":"2025-10-15T21:51:29.048981Z","iopub.status.idle":"2025-10-15T21:51:29.073350Z","shell.execute_reply.started":"2025-10-15T21:51:29.048932Z","shell.execute_reply":"2025-10-15T21:51:29.072356Z"}},"outputs":[{"name":"stdout","text":"Training samples: 625\nTest samples: 157\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"print(\"First 5 samples from PyTorch training dataset (after fix):\")\nfor i in range(5):\n    features, targets = train_dataset[i]\n    print(f\"Sample {i}:\")\n    print(f\"  Features: {features}\")\n    print(f\"  Target: {targets}\")\n    print(\"-\" * 20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T21:51:29.074574Z","iopub.execute_input":"2025-10-15T21:51:29.074847Z","iopub.status.idle":"2025-10-15T21:51:29.091236Z","shell.execute_reply.started":"2025-10-15T21:51:29.074825Z","shell.execute_reply":"2025-10-15T21:51:29.090020Z"}},"outputs":[{"name":"stdout","text":"First 5 samples from PyTorch training dataset (after fix):\nSample 0:\n  Features: tensor([ 1.6476,  0.2107,  0.6865,  0.1282,  0.2398, -0.5858,  0.1482, -0.4134,\n        -0.1418,  0.6836, -0.5454, -1.5931])\n  Target: 0\n--------------------\nSample 1:\n  Features: tensor([ 1.8660,  1.4617,  2.0595,  2.0106, -0.9163, -0.3673, -0.1909, -0.4276,\n        -1.6856,  1.0256,  0.5990,  1.2085])\n  Target: 1\n--------------------\nSample 2:\n  Features: tensor([ 1.8660,  1.1489,  2.0595,  5.8387, -0.9163,  0.2578, -0.5379, -0.4791,\n         0.9030,  0.2736,  0.4355, -0.7526])\n  Target: 0\n--------------------\nSample 3:\n  Features: tensor([-0.5367, -0.1021, -0.6865, -0.4813, -0.9163,  1.1136, -0.1524, -0.4508,\n         0.0839, -1.1136,  0.1085,  0.3680])\n  Target: 1\n--------------------\nSample 4:\n  Features: tensor([-0.3182, -1.3531,  0.0000, -0.4933,  0.2558, -0.5858, -1.0004, -0.3543,\n         0.4928, -1.3066, -1.6897, -0.7526])\n  Target: 0\n--------------------\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\n\nclass TsunamiNN(nn.Module):\n    def __init__(self, input_size, hidden_sizes, num_classes):\n        super(TsunamiNN, self).__init__()\n        \n        layers = []\n        prev_size = input_size\n        \n        for hidden_size in hidden_sizes:\n            layers.append(nn.Linear(prev_size, hidden_size))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(0.3)) \n            prev_size = hidden_size\n\n        layers.append(nn.Linear(prev_size, num_classes))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\n\ndef train_model(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for features, labels in train_loader:\n        features, labels = features.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\n\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for features, labels in test_loader:\n            features, labels = features.to(device), labels.to(device)\n            \n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(test_loader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ninput_size = 12  # Number of features\nhidden_sizes = [64, 32, 16]  # Hidden layer sizes\nnum_classes = 2  # Binary classification (tsunami or not)\n\nmodel = TsunamiNN(input_size, hidden_sizes, num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 50\nbest_test_acc = 0.0\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n    \n    # Save best model\n    if test_acc > best_test_acc:\n        best_test_acc = test_acc\n        torch.save(model.state_dict(), 'best_tsunami_model.pth')\n    \n    if (epoch + 1) % 5 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}]')\n        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n\nprint(f'\\n   Best Test Accuracy: {best_test_acc:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T21:51:29.092233Z","iopub.execute_input":"2025-10-15T21:51:29.092548Z","iopub.status.idle":"2025-10-15T21:51:31.024840Z","shell.execute_reply.started":"2025-10-15T21:51:29.092527Z","shell.execute_reply":"2025-10-15T21:51:31.023868Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nStarting training...\nEpoch [5/50]\n  Train Loss: 0.4263, Train Acc: 80.32%\n  Test Loss: 0.3865, Test Acc: 84.08%\nEpoch [10/50]\n  Train Loss: 0.3628, Train Acc: 83.52%\n  Test Loss: 0.3382, Test Acc: 84.08%\nEpoch [15/50]\n  Train Loss: 0.3427, Train Acc: 85.28%\n  Test Loss: 0.3184, Test Acc: 85.99%\nEpoch [20/50]\n  Train Loss: 0.3220, Train Acc: 84.32%\n  Test Loss: 0.3034, Test Acc: 85.99%\nEpoch [25/50]\n  Train Loss: 0.3024, Train Acc: 85.76%\n  Test Loss: 0.2837, Test Acc: 87.26%\nEpoch [30/50]\n  Train Loss: 0.3106, Train Acc: 85.76%\n  Test Loss: 0.2726, Test Acc: 89.17%\nEpoch [35/50]\n  Train Loss: 0.2894, Train Acc: 85.44%\n  Test Loss: 0.2644, Test Acc: 87.90%\nEpoch [40/50]\n  Train Loss: 0.2778, Train Acc: 87.20%\n  Test Loss: 0.2681, Test Acc: 87.90%\nEpoch [45/50]\n  Train Loss: 0.2584, Train Acc: 87.84%\n  Test Loss: 0.2575, Test Acc: 89.81%\nEpoch [50/50]\n  Train Loss: 0.2460, Train Acc: 88.80%\n  Test Loss: 0.2568, Test Acc: 89.81%\n\nBest Test Accuracy: 90.45%\n","output_type":"stream"}],"execution_count":57}]}