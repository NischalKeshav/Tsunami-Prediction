{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13229155,"sourceType":"datasetVersion","datasetId":8385511}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"ahmeduzaki/global-earthquake-tsunami-risk-assessment-dataset\")\n\nprint(\"Path to dataset files:\", path) #be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:53:06.164621Z","iopub.execute_input":"2025-10-15T20:53:06.165060Z","iopub.status.idle":"2025-10-15T20:53:06.392880Z","shell.execute_reply.started":"2025-10-15T20:53:06.165031Z","shell.execute_reply":"2025-10-15T20:53:06.391846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/global-earthquake-tsunami-risk-assessment-dataset/earthquake_data_tsunami.csv')\nprint(\"Complete\")\n\nX= df[['magnitude', 'cdi', 'mmi', 'sig', 'nst', 'dmin', 'gap', 'depth',\n                 'latitude', 'longitude', 'Year', 'Month']]\ny = 'tsunami'\n\ntrain_df, test_df = train_test_split(\n    df, test_size=0.25, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:53:06.395009Z","iopub.execute_input":"2025-10-15T20:53:06.395354Z","iopub.status.idle":"2025-10-15T20:53:06.408992Z","shell.execute_reply.started":"2025-10-15T20:53:06.395330Z","shell.execute_reply":"2025-10-15T20:53:06.407634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDatasetFromDataFrame(Dataset):\n    def __init__(self, dataframe, features_cols, target_col, mean=None, std=None):\n        # Change to float32 explicitly\n        self.features = torch.tensor(dataframe[features_cols].values, dtype=torch.float32)\n        self.targets = torch.tensor(dataframe[target_col].values, dtype=torch.long)\n        \n        if mean is not None and std is not None:\n            # Ensure mean and std are also float32\n            mean = torch.tensor(mean.values, dtype=torch.float32)\n            std = torch.tensor(std.values, dtype=torch.float32)\n            self.features = (self.features - mean) / (std + 1e-8)\n    \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.targets[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:53:06.410045Z","iopub.execute_input":"2025-10-15T20:53:06.410269Z","iopub.status.idle":"2025-10-15T20:53:06.421716Z","shell.execute_reply.started":"2025-10-15T20:53:06.410247Z","shell.execute_reply":"2025-10-15T20:53:06.420587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming you have already created your train_df\n# train_df, test_df = train_test_split(df, ...)\n\n# Select the feature columns from the training data\nfeatures_cols = ['magnitude', 'cdi', 'mmi', 'sig', 'nst', 'dmin', 'gap', 'depth',\n                 'latitude', 'longitude', 'Year', 'Month']\n\n# Calculate mean and standard deviation for each feature\ntrain_mean = train_df[features_cols].mean()\ntrain_std = train_df[features_cols].std()\n\n# Handle cases where standard deviation might be zero (for constant features)\n# Replace zero std with 1 to avoid division by zero\ntrain_std[train_std == 0] = 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:53:06.422800Z","iopub.execute_input":"2025-10-15T20:53:06.423202Z","iopub.status.idle":"2025-10-15T20:53:06.444479Z","shell.execute_reply.started":"2025-10-15T20:53:06.423176Z","shell.execute_reply":"2025-10-15T20:53:06.443374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. FIRST: Define columns\nfeatures_cols = ['magnitude', 'cdi', 'mmi', 'sig', 'nst', 'dmin', 'gap', 'depth',\n                 'latitude', 'longitude', 'Year', 'Month']\ntarget_col = 'tsunami'\n\n# 2. SECOND: Split the data\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target_col])\n\n# 3. THIRD: Calculate mean and std from training data\ntrain_mean = train_df[features_cols].mean()\ntrain_std = train_df[features_cols].std()\n\n# 4. FINALLY: Create the datasets\ntrain_dataset = CustomDatasetFromDataFrame(\n    train_df, \n    features_cols, \n    target_col, \n    mean=train_mean,\n    std=train_std\n)\ntest_dataset = CustomDatasetFromDataFrame(\n    test_df, \n    features_cols, \n    target_col, \n    mean=train_mean, \n    std=train_std\n)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:53:06.446633Z","iopub.execute_input":"2025-10-15T20:53:06.447722Z","iopub.status.idle":"2025-10-15T20:53:06.467397Z","shell.execute_reply.started":"2025-10-15T20:53:06.447689Z","shell.execute_reply":"2025-10-15T20:53:06.466535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rerun the verification loop\nprint(\"First 5 samples from PyTorch training dataset (after fix):\")\nfor i in range(5):\n    features, targets = train_dataset[i]\n    print(f\"Sample {i}:\")\n    print(f\"  Features: {features}\")\n    print(f\"  Target: {targets}\")\n    print(\"-\" * 20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:53:06.468232Z","iopub.execute_input":"2025-10-15T20:53:06.468588Z","iopub.status.idle":"2025-10-15T20:53:06.488766Z","shell.execute_reply.started":"2025-10-15T20:53:06.468564Z","shell.execute_reply":"2025-10-15T20:53:06.487983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\n\nclass TsunamiNN(nn.Module):\n    def __init__(self, input_size, hidden_sizes, num_classes):\n        super(TsunamiNN, self).__init__()\n        \n        layers = []\n        prev_size = input_size\n        \n        # Create hidden layers\n        for hidden_size in hidden_sizes:\n            layers.append(nn.Linear(prev_size, hidden_size))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(0.3)) \n            prev_size = hidden_size\n\n        layers.append(nn.Linear(prev_size, num_classes))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\n\ndef train_model(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for features, labels in train_loader:\n        features, labels = features.to(device), labels.to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        \n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\n\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for features, labels in test_loader:\n            features, labels = features.to(device), labels.to(device)\n            \n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(test_loader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ninput_size = 12  # Number of features\nhidden_sizes = [64, 32, 16]  # Hidden layer sizes\nnum_classes = 2  # Binary classification (tsunami or not)\n\nmodel = TsunamiNN(input_size, hidden_sizes, num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 50\nbest_test_acc = 0.0\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n    \n    # Save best model\n    if test_acc > best_test_acc:\n        best_test_acc = test_acc\n        torch.save(model.state_dict(), 'best_tsunami_model.pth')\n    \n    if (epoch + 1) % 5 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}]')\n        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n\nprint(f'\\nBest Test Accuracy: {best_test_acc:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:53:06.489561Z","iopub.execute_input":"2025-10-15T20:53:06.489762Z","iopub.status.idle":"2025-10-15T20:53:08.400401Z","shell.execute_reply.started":"2025-10-15T20:53:06.489746Z","shell.execute_reply":"2025-10-15T20:53:08.399218Z"}},"outputs":[],"execution_count":null}]}